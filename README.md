# Ultralight Digital Human

<p align="center">
    <a href="./LICENSE"><img src="https://img.shields.io/badge/license-Apache%202-dfd.svg"></a>
    <a href=""><img src="https://img.shields.io/badge/python-3.10-aff.svg"></a>
    <a href="https://github.com/anliyuan/Ultralight-Digital-Human/stargazers"><img src="https://img.shields.io/github/stars/anliyuan/Ultralight-Digital-Human?color=ccf"></a>
  <br>
    <br>
</p>

A Ultralight Digital Human model can run on mobile devices in real time!!!

ä¸€ä¸ªèƒ½åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œçš„æ•°å­—äººæ¨¡å‹,æ®æˆ‘æ‰€çŸ¥ï¼Œè¿™åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªå¼€æºçš„å¦‚æ­¤è½»é‡çº§çš„æ•°å­—äººæ¨¡å‹ã€‚

Lets see the demo.â¬‡ï¸â¬‡ï¸â¬‡ï¸

å…ˆæ¥çœ‹ä¸ªdemoâ¬‡ï¸â¬‡ï¸â¬‡ï¸

![DigitalHuman](https://github.com/user-attachments/assets/9d0b37ee-2076-4b4f-93ba-eb939a9fb427)


## Train

It's so easy to train your own digital human.I will show you step by step.

è®­ç»ƒä¸€ä¸ªä½ è‡ªå·±çš„æ•°å­—äººéå¸¸ç®€å•ï¼Œæˆ‘å°†ä¸€æ­¥æ­¥å‘ä½ å±•ç¤ºã€‚

### install pytorch and other libs

``` bash
conda create -n dh python=3.10
conda activate dh
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia
conda install mkl=2024.0
pip install opencv-python
pip install transformers
pip install numpy==1.23.5
pip install soundfile
pip install librosa
pip install onnxruntime
```

I only ran on pytorch==1.13.1, Other versions should also work.

æˆ‘æ˜¯åœ¨1.13.1ç‰ˆæœ¬çš„pytorchè·‘çš„ï¼Œå…¶ä»–ç‰ˆæœ¬çš„pytorchåº”è¯¥ä¹Ÿå¯ä»¥ã€‚

Download wenet encoder.onnx from https://drive.google.com/file/d/1e4Z9zS053JEWl6Mj3W9Lbc9GDtzHIg6b/view?usp=drive_link 

and put it in data_utils/

### Data preprocessing

Prepare your video, 3~5min is good. Make sure that every frame of the video has the person's full face exposed and the sound is clear without any noise, put it in a new folder.I will provide a demo video.

å‡†å¤‡å¥½ä½ çš„è§†é¢‘ï¼Œ3åˆ°5åˆ†é’Ÿçš„å°±å¯ä»¥ï¼Œå¿…é¡»ä¿è¯è§†é¢‘ä¸­æ¯ä¸€å¸§éƒ½æœ‰æ•´å¼ è„¸éœ²å‡ºæ¥çš„äººç‰©ï¼Œå£°éŸ³æ¸…æ™°æ²¡æœ‰æ‚éŸ³ï¼ŒæŠŠå®ƒæ”¾åˆ°ä¸€ä¸ªæ–°çš„æ–‡ä»¶å¤¹é‡Œé¢ã€‚æˆ‘ä¼šæä¾›ä¸€ä¸ªdemoè§†é¢‘ï¼Œæ¥è‡ªåº·è¾‰è€å¸ˆçš„å£æ’­ï¼Œä¾µåˆ ã€‚

First of all, we need to extract audio feature.I'm using 2 different extractor from wenet and hubert, thank them for their great work.

wenetçš„ä»£ç å’Œä¸è®­ç»ƒæ¨¡å‹æ¥è‡ª:https://github.com/Tzenthin/wenet_mnn

é¦–å…ˆæˆ‘ä»¬éœ€è¦æå–éŸ³é¢‘ç‰¹å¾ï¼Œæˆ‘ç”¨äº†ä¸¤ä¸ªä¸åŒçš„ç‰¹å¾æå–èµ·ï¼Œåˆ†åˆ«æ˜¯wenetå’Œhubertï¼Œæ„Ÿè°¢ä»–ä»¬ã€‚

When you using wenet, you neet to ensure that your video frame rate is 20, and for hubert,your video frame rate should be 25.

å¦‚æœä½ é€‰æ‹©ä½¿ç”¨wenetçš„è¯ï¼Œä½ å¿…é¡»ä¿è¯ä½ è§†é¢‘çš„å¸§ç‡æ˜¯20fpsï¼Œå¦‚æœé€‰æ‹©hubertï¼Œè§†é¢‘å¸§ç‡å¿…é¡»æ˜¯25fpsã€‚

In my experiments, hubert performs better, but wenet is faster and can run in real time on mobile devices.

åœ¨æˆ‘çš„å®éªŒä¸­ï¼Œhubertçš„æ•ˆæœæ›´å¥½ï¼Œä½†æ˜¯weneté€Ÿåº¦æ›´å¿«ï¼Œå¯ä»¥åœ¨ç§»åŠ¨ç«¯ä¸Šå®æ—¶è¿è¡Œ

And other steps are in data_utils/process.py, you just run it like this.

å…¶ä»–æ­¥éª¤éƒ½å†™åœ¨data_utils/process.pyé‡Œé¢äº†ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«è¦æ³¨æ„çš„ã€‚

``` bash
cd data_utils
python process.py YOUR_VIDEO_PATH --asr hubert
```

Then you wait.

ç„¶åç­‰å®ƒè¿è¡Œå®Œå°±è¡Œäº†

### train

After the preprocessing step, you can start training the model.

ä¸Šé¢æ­¥éª¤ç»“æŸåï¼Œå°±å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ã€‚

Train a syncnet first for better results.

å…ˆè®­ç»ƒä¸€ä¸ªsyncnetï¼Œæ•ˆæœä¼šæ›´å¥½ã€‚

``` bash
cd ..
python syncnet.py --save_dir ./syncnet_ckpt/ --dataset_dir ./data_dir/ --asr hubert
```

Then find a best oneï¼ˆlow lossï¼‰ to train digital human model.

ç„¶åæ‰¾ä¸€ä¸ªlossæœ€ä½çš„checkpointæ¥è®­ç»ƒæ•°å­—äººæ¨¡å‹ã€‚

``` bash
cd ..
python train.py --dataset_dir ./data_dir/ --save_dir ./checkpoint/ --asr hubert --use_syncnet --syncnet_checkpoint syncnet_ckpt
```

## inference

Before run inference, you need to extract test audio feature(i will merge this step and inference step), run this

åœ¨æ¨ç†ä¹‹å‰ï¼Œéœ€è¦å…ˆæå–æµ‹è¯•éŸ³é¢‘çš„ç‰¹å¾ï¼ˆä¹‹åä¼šæŠŠè¿™æ­¥å’Œæ¨ç†åˆå¹¶åˆ°ä¸€èµ·å»ï¼‰ï¼Œè¿è¡Œ

``` bash
python data_utils/hubert.py --wav your_test_audio.wav  # when using hubert

or

python data_utils/python wenet_infer.py your_test_audio.wav  # when using wenet
```

then you get your_test_audio_hu.npy or your_test_audio_wenet.npy

then run
``` bash
python inference.py --asr hubert --dataset ./your_data_dir/ --audio_feat your_test_audio_hu.npy --save_path xxx.mp4 --checkpoint your_trained_ckpt.pth
```

To merge the audio and the video, run

``` bash
ffmpeg -i xxx.mp4 -i your_audio.wav -c:v libx264 -c:a aac result_test.mp4
```

## EnjoyğŸ‰ğŸ‰ğŸ‰

è¿™ä¸ªæ¨¡å‹æ˜¯æ”¯æŒæµå¼æ¨ç†çš„ï¼Œä½†æ˜¯ä»£ç è¿˜æ²¡æœ‰å®Œå–„ï¼Œä¹‹åæˆ‘ä¼šæä¸Šæ¥ã€‚

å…³äºåœ¨ç§»åŠ¨ç«¯ä¸Šè¿è¡Œä¹Ÿæ˜¯æ²¡é—®é¢˜çš„ï¼Œåªéœ€è¦æŠŠç°åœ¨è¿™ä¸ªæ¨¡å‹é€šé“æ•°æ”¹å°ä¸€ç‚¹ï¼ŒéŸ³é¢‘ç‰¹å¾ç”¨wenetå°±æ²¡é—®é¢˜äº†ã€‚ç›¸å…³ä»£ç æˆ‘ä¹Ÿä¼šåœ¨ä¹‹åæ”¾ä¸Šæ¥ã€‚

if you have some advice, open an issue or PR.

å¦‚æœä½ æœ‰æ”¹è¿›çš„å»ºè®®ï¼Œå¯ä»¥æä¸ªissueæˆ–è€…PRã€‚

If you think this repo is useful to you, please give me a star.

å¦‚æœä½ è§‰çš„è¿™ä¸ªrepoå¯¹ä½ æœ‰ç”¨çš„è¯ï¼Œè®°å¾—ç»™æˆ‘ç‚¹ä¸ªstar

BUY ME A CUP OF COFFEâ¬‡ï¸â¬‡ï¸â¬‡ï¸
<table>
  <tr>
    <td><img src="demo/15bef5a6d08434c0d70f0ba39bb14fc0.JPG" width="180"/></td>
    <td><img src="demo/36d2896f13bee68247de6ccc89b17a94.JPG" width="180"/></td>
  </tr>
</table>
